#!/usr/bin/env python
# coding: utf-8

# In[3]:


# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv("Bank_Personal_Loan_Modelling.csv")

# Display the first few rows of the dataset
print(data.head())


# In[4]:


# Visualize the distribution of Age
plt.figure(figsize=(8, 6))
sns.histplot(data['Age'], kde=True, color='green')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Visualize the distribution of Income
plt.figure(figsize=(8, 6))
sns.histplot(data['Income'], kde=True, color='orange')
plt.title('Distribution of Income')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

# Visualize the distribution of CCAvg
plt.figure(figsize=(8, 6))
sns.histplot(data['CCAvg'], kde=True, color='blue')
plt.title('Distribution of CCAvg')
plt.xlabel('CCAvg')
plt.ylabel('Frequency')
plt.show()

# Visualize the distribution of Mortgage
plt.figure(figsize=(8, 6))
sns.histplot(data['Mortgage'], kde=True, color='red')
plt.title('Distribution of Mortgage')
plt.xlabel('Mortgage')
plt.ylabel('Frequency')
plt.show()


# In[5]:


# Calculate the correlation matrix
corr_matrix = data.corr()

# Plot the correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()


# In[6]:


from sklearn.model_selection import train_test_split

# Drop irrelevant columns and split features (X) and target variable (y)
X = data.drop(columns=['Personal Loan'])
y = data['Personal Loan']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the training and testing sets
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)


# In[7]:


from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize models
logistic_model = LogisticRegression(random_state=42)
decision_tree_model = DecisionTreeClassifier(random_state=42)
random_forest_model = RandomForestClassifier(random_state=42)

# Train models
logistic_model.fit(X_train, y_train)
decision_tree_model.fit(X_train, y_train)
random_forest_model.fit(X_train, y_train)

# Predictions
y_pred_logistic = logistic_model.predict(X_test)
y_pred_dt = decision_tree_model.predict(X_test)
y_pred_rf = random_forest_model.predict(X_test)

# Evaluation
accuracy_logistic = accuracy_score(y_test, y_pred_logistic)
accuracy_dt = accuracy_score(y_test, y_pred_dt)
accuracy_rf = accuracy_score(y_test, y_pred_rf)

print("Logistic Regression Accuracy:", accuracy_logistic)
print("Decision Tree Accuracy:", accuracy_dt)
print("Random Forest Accuracy:", accuracy_rf)

# Print classification report for logistic regression
print("\nLogistic Regression Classification Report:")
print(classification_report(y_test, y_pred_logistic))


# In[9]:


import warnings
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Suppress convergence warnings
warnings.filterwarnings("ignore")

# Initialize logistic regression model
logistic_model = LogisticRegression(random_state=42)

# Initialize RFE
rfe = RFE(estimator=logistic_model, n_features_to_select=5)

# Fit RFE on training data
rfe.fit(X_train, y_train)

# Get selected features
selected_features = X_train.columns[rfe.support_]

print("Selected Features:", selected_features)


# In[10]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42)
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy:", accuracy)
    print(f"{name} Classification Report:")
    print(classification_report(y_test, y_pred))


# In[12]:


# Define the parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter for logistic regression
    'solver': ['liblinear', 'lbfgs'],  # Solver for logistic regression
}

# Initialize the model for hyperparameter tuning
logistic_model = LogisticRegression(random_state=42)

# Perform grid search cross-validation to find the best hyperparameters
grid_search = GridSearchCV(logistic_model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Use the best hyperparameters to train the final model
final_model = LogisticRegression(**best_params, random_state=42)
final_model.fit(X_train, y_train)

# Evaluate the final model
y_pred_final = final_model.predict(X_test)
accuracy_final = accuracy_score(y_test, y_pred_final)
print("Final Model Accuracy:", accuracy_final)
print("Final Model Classification Report:")
print(classification_report(y_test, y_pred_final))


# In[13]:


from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix for the final model
cm_final = confusion_matrix(y_test, y_pred_final)

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_final, annot=True, fmt="d", cmap="Oranges", cbar=False)
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.title("Confusion Matrix (Final Model)")
plt.show()


# In[ ]:




